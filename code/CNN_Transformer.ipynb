{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mount on googledrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is already stored \n",
    "import joblib\n",
    "data = joblib.load('/content/drive/MyDrive/DeepSTARR_tutorial/data')\n",
    "\n",
    "X_train = joblib.load('/content/drive/MyDrive/DeepSTARR_tutorial/X_train')\n",
    "Y_train = joblib.load('/content/drive/MyDrive/DeepSTARR_tutorial/Y_train')\n",
    "\n",
    "X_valid = joblib.load('/content/drive/MyDrive/DeepSTARR_tutorial/X_valid')\n",
    "Y_valid = joblib.load('/content/drive/MyDrive/DeepSTARR_tutorial/Y_valid')\n",
    "\n",
    "X_test = joblib.load('/content/drive/MyDrive/DeepSTARR_tutorial/X_test')\n",
    "Y_test = joblib.load('/content/drive/MyDrive/DeepSTARR_tutorial/Y_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implementation (CNN + Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as kl\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "params_smaller_data = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 1,\n",
    "    'early_stop': 20,\n",
    "    'lr': 0.001,\n",
    "    'n_conv_layer': 3,\n",
    "    'num_filters1': 128,\n",
    "    'num_filters2': 60,\n",
    "    'num_filters3': 60,\n",
    "    'kernel_size1': 7,\n",
    "    'kernel_size2': 3,\n",
    "    'kernel_size3': 5,\n",
    "    'n_dense_layer': 1,\n",
    "    'dense_neurons1': 64,\n",
    "    'dropout_conv': 'yes',\n",
    "    'dropout_prob': 0.4,\n",
    "    'pad': 'same',\n",
    "    'num_heads': 4,  # number of attention heads\n",
    "    'dff': 128,  # depth of the feed-forward network\n",
    "    'sequence_length': 249  # adjust based on your data\n",
    "}\n",
    "\n",
    "\n",
    "def positional_encoding(positions, d_model):\n",
    "    positions = tf.range(positions, dtype=tf.float32)[:, tf.newaxis]  # Explicitly cast to float32\n",
    "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]  # Explicitly cast to float32\n",
    "    angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    angle_rads = positions * angle_rates\n",
    "    sines = tf.sin(angle_rads[:, 0::2])\n",
    "    cosines = tf.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    return pos_encoding\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0, position=None):\n",
    "    effective_length = inputs.shape[1]\n",
    "    pos_encoding = positional_encoding(effective_length, inputs.shape[-1])\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, 0)\n",
    "    x = kl.Add()([inputs,pos_encoding])  # Add positional encoding to the transformer input\n",
    "\n",
    "    # First MultiHeadAttention Layer\n",
    "    x = kl.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = inputs\n",
    "    x = kl.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Add()([x,inputs])  # Skip Connection\n",
    "\n",
    "    # Second MultiHeadAttention Layer\n",
    "    x = kl.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = kl.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Add()([x,inputs])  # Skip Connection again\n",
    "\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = kl.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = kl.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Dense(inputs.shape[-1])(x)\n",
    "    return kl.Add()([x,inputs])  # Skip Connection\n",
    "\n",
    "def DeepSTARR(params):\n",
    "    input = kl.Input(shape=(params['sequence_length'], 4))\n",
    "\n",
    "    # Convolutional layers as before\n",
    "    x = kl.Conv1D(params['num_filters1'], kernel_size=params['kernel_size1'],\n",
    "                  padding=params['pad'], name='Conv1D_1')(input)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Activation('relu')(x)\n",
    "    x = kl.MaxPooling1D(2)(x)\n",
    "\n",
    "    for i in range(1, params['n_conv_layer']):\n",
    "        x = kl.Conv1D(params['num_filters' + str(i + 1)],\n",
    "                      kernel_size=params['kernel_size' + str(i + 1)],\n",
    "                      padding=params['pad'], name='Conv1D_' + str(i + 1))(x)\n",
    "        x = kl.BatchNormalization()(x)\n",
    "        x = kl.Activation('relu')(x)\n",
    "        x = kl.MaxPooling1D(2)(x)\n",
    "        if params['dropout_conv'] == 'yes':\n",
    "            x = kl.Dropout(params['dropout_prob'])(x)\n",
    "\n",
    "    # Transformer layer with positional encoding\n",
    "    x = transformer_encoder(x, head_size=64, num_heads=params['num_heads'], ff_dim=params['dff'], dropout=params['dropout_prob'], position=params['sequence_length']//4)\n",
    "\n",
    "    # Dense layers \n",
    "    x = kl.Flatten()(x)\n",
    "    for i in range(0, params['n_dense_layer']):\n",
    "        x = kl.Dense(params['dense_neurons' + str(i + 1)], name='Dense_' + str(i + 1))(x)\n",
    "        x = kl.BatchNormalization()(x)\n",
    "        x = kl.Activation('relu')(x)\n",
    "        x = kl.Dropout(params['dropout_prob'])(x)\n",
    "\n",
    "    bottleneck = x\n",
    "    tasks = ['Dev', 'Hk']\n",
    "    outputs = []\n",
    "    for task in tasks:\n",
    "        outputs.append(kl.Dense(1, activation='linear', name='Dense_' + task)(bottleneck))\n",
    "\n",
    "    model = Model([input], outputs)\n",
    "    model.compile(Adam(learning_rate=params['lr']),\n",
    "                  loss=['mse', 'mse'],\n",
    "                  loss_weights=[1, 1])\n",
    "\n",
    "    return model, params\n",
    "\n",
    "model, params = DeepSTARR(params_smaller_data)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to to train model\n",
    "def train(selected_model, X_train, Y_train, X_valid, Y_valid, params):\n",
    "\n",
    "    my_history=selected_model.fit(X_train, Y_train,\n",
    "                                  validation_data=(X_valid, Y_valid), # The function trains the selected_model using the training data and validates it using the validation data.\n",
    "                                  batch_size=params['batch_size'],\n",
    "                                  epochs=params['epochs'],\n",
    "                                  callbacks=[EarlyStopping(patience=params['early_stop'], monitor=\"val_loss\", restore_best_weights=True), # The training will stop early if there is no improvement in validation loss after a certain number of epochs (patience)\n",
    "                                             History()])\n",
    "\n",
    "    # save model and history\n",
    "    return selected_model, my_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model, main_params = DeepSTARR(params_smaller_data) \n",
    "main_model, my_history = train(main_model, X_train, Y_train, X_valid, Y_valid, main_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation training metrics\n",
    "for out in ['Dev', 'Hk']:\n",
    "  # MSE\n",
    "  plt.plot(my_history.history[str('Dense_' + out + '_loss')])\n",
    "  plt.plot(my_history.history[str('val_Dense_' + out + '_loss')])\n",
    "  plt.title(out) # loss is Mean Squared Error (MSE)\n",
    "  plt.ylabel('Loss MSE')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "  # Add vertical line at minimum validation loss of combined dev and hk\n",
    "  min_val_loss = min(my_history.history['val_loss'])\n",
    "  plt.axvline(x=my_history.history['val_loss'].index(min_val_loss), color='red', linestyle='--')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance metrics\n",
    "\n",
    "Analyse Mean Squared Error (MSE) and Pearson (PCC) and Spearman (SCC) correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create functions\n",
    "def summary_statistics(X, Y, set, task):\n",
    "    pred = main_model.predict(X, batch_size=main_params['batch_size']) # predict\n",
    "    if task ==\"Dev\":\n",
    "        i=0\n",
    "    if task ==\"Hk\":\n",
    "        i=1\n",
    "    print(set + ' MSE ' + task + ' = ' + str(\"{0:0.2f}\".format(mean_squared_error(Y, pred[i].squeeze()))))\n",
    "    print(set + ' PCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.pearsonr(Y, pred[i].squeeze())[0])))\n",
    "    # print(set + ' SCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.spearmanr(Y, pred[i].squeeze())[0]))) # not giving similar results as training\n",
    "\n",
    "# predict activity for each set and enhancer type\n",
    "summary_statistics(X_train, Y_train[0], \"train\", \"Dev\")\n",
    "summary_statistics(X_valid, Y_valid[0], \"validation\", \"Dev\")\n",
    "summary_statistics(X_test, Y_test[0], \"test\", \"Dev\")\n",
    "\n",
    "summary_statistics(X_train, Y_train[1], \"train\", \"Hk\")\n",
    "summary_statistics(X_valid, Y_valid[1], \"validation\", \"Hk\")\n",
    "summary_statistics(X_test, Y_test[1], \"test\", \"Hk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is some overfitting (i.e. the performance is better in the training set than test set), the performance on the test data is still good. This suggests that the model accurately captures the regulatory information present in the DNA sequences, as you will see in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots\n",
    "The scatter plots compare the observed values with the predicted ones for each set of sequences (train/validation/testing). This allows to compare the global performance of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scatter(X, Y, set, task):\n",
    "  pred = main_model.predict(X, batch_size=main_params['batch_size'])\n",
    "  if task ==\"Dev\":\n",
    "    i=0\n",
    "    c=\"red\"\n",
    "  if task ==\"Hk\":\n",
    "    i=1\n",
    "    c=\"blue\"\n",
    "\n",
    "  g = sns.regplot(x=Y, y=pred[i].squeeze(), ci=None, color=c,\n",
    "                  fit_reg=False,\n",
    "                  scatter_kws={'s': 5, 'alpha': 0.2},\n",
    "                  line_kws={'color': \"black\"})\n",
    "\n",
    "  # add expected regression line\n",
    "  x0, x1 = g.get_xlim()\n",
    "  y0, y1 = g.get_ylim()\n",
    "  lims = [max(x0, y0), min(x1, y1)]\n",
    "  g.plot(lims, lims, 'w', linestyle='dashed', transform=g.transData, color='grey')\n",
    "\n",
    "  # same axes ranges\n",
    "  g.set_aspect('equal')\n",
    "  # g.set(xlim=(min(x0, y0), max(x1, y1)),\n",
    "  #       ylim=(min(x0, y0), max(x1, y1)))\n",
    "\n",
    "  PCC = str(\"{0:0.2f}\".format(stats.pearsonr(Y, pred[i].squeeze())[0]))\n",
    "  plt.xlabel('Measured expression [log2]')\n",
    "  plt.ylabel('Predicted expression [log2]')\n",
    "  plt.title(str(task + ' - ' + set + ' set (PCC=' + PCC + ')'))\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "# plots\n",
    "my_scatter(X_test, Y_test[0], \"test\", \"Dev\")\n",
    "my_scatter(X_test, Y_test[1], \"test\", \"Hk\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
